import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("üë®‚Äçüíºüí∏üë©‚Äçüíº –ê–Ω–∞–ª–∏–∑ –∑–∞—è–≤–æ–∫ –Ω–∞ –∫—Ä–µ–¥–∏—Ç")
st.write(
    """
    –≠—Ç–æ Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–æ–±—Ä–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–æ–≤. 
    –í–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é 3D-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –≥—Ä–∞–Ω–∏—Ü —Ä–µ—à–µ–Ω–∏–π.
    """
)

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")
test_size = st.sidebar.slider("–î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏", 0.1, 0.3, 0.2, 0.05)
k_neighbors = st.sidebar.slider("–ß–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π (kNN)", 1, 15, 3, 1)
max_depth = st.sidebar.slider("–ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞", 1, 20, 5, 1)
max_iter = st.sidebar.slider("–ú–∞–∫—Å. –∏—Ç–µ—Ä–∞—Ü–∏–π (Logistic Regression)", 100, 1000, 500, 5)

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
@st.cache_data
def load_data():
    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data'
    df = pd.read_csv(url, header=None, na_values='?')

    df = df.dropna(subset=[15])  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ

    for col in df.columns:
        if col != 15 and df[col].dtype == 'object':
            df[col] = pd.to_numeric(df[col], errors='coerce')

    df.dropna(axis=1, how='all', inplace=True)  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    num_cols = [col for col in df.columns if col != 15 and pd.api.types.is_numeric_dtype(df[col])]
    df[num_cols] = df.groupby(15)[num_cols].transform(lambda x: x.fillna(x.mean()))  # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º –ø–æ –∫–ª–∞—Å—Å—É
    df[15] = df[15].map({'+': 1, '-': 0})  # –ü–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞ —Ç–∞—Ä–≥–µ—Ç–∞
    return df

data = load_data()
st.subheader("–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
st.write(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
st.dataframe(data.head(10))

# –í—ã–±–æ—Ä 3 –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
num_features = [col for col in data.columns if col != 15 and data[col].nunique() > 10]
corr = {col: data[col].corr(data[15]) for col in num_features}
sorted_features = sorted(corr.items(), key=lambda x: abs(x[1]), reverse=True)
top3_features = sorted_features[:3]

st.write("**–¢—Ä–∏ —Å–∞–º—ã—Ö –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ (–ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏):**")
for feature, corr_value in top3_features:
    st.write(f"–ü—Ä–∏–∑–Ω–∞–∫ {feature}: {corr_value:.3f}")

# –í—ã–±–æ—Ä 2 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
st.sidebar.subheader("–í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
numeric_cols = [col for col in data.columns if col != 15 and pd.api.types.is_numeric_dtype(data[col])]
selected_features = st.sidebar.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞", options=numeric_cols, default=numeric_cols[:2])

if len(selected_features) != 2:
    st.error("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–æ–≤–Ω–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞!")
    st.stop()

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X = data[selected_features]
y = data[15]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
st.subheader("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
models = [
    ("kNN", KNeighborsClassifier(n_neighbors=k_neighbors)),
    ("Logistic Regression", LogisticRegression(max_iter=max_iter)),
    ("Decision Tree", DecisionTreeClassifier(max_depth=max_depth, random_state=42))
]

results = {}
for name, model in models:
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)

    # ROC-–∫—Ä–∏–≤–∞—è
    fpr, tpr, roc_auc = None, None, None
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)

    results[name] = {"model": model, "accuracy": acc, "fpr": fpr, "tpr": tpr, "roc_auc": roc_auc}
    st.write(f"**{name}** ‚Äî Accuracy: {acc:.3f}, AUC: {roc_auc:.3f}" if roc_auc else f"**{name}** ‚Äî Accuracy: {acc:.3f}")

# –ì—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π (2D)
st.subheader("–ì—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π")
fig, axes = plt.subplots(3, 1, figsize=(8, 12))

y_train_array = y_train.values  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∞—Å—Å–∏–≤ –¥–ª—è plot_decision_regions
for i, (name, model) in enumerate(models):
    ax = axes[i]
    plot_decision_regions(X_train_scaled, y_train_array, clf=model, legend=2, ax=ax)
    ax.set_xlabel(str(selected_features[0]))
    ax.set_ylabel(str(selected_features[1]))
    ax.set_title(f"–ì—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π: {name}")

plt.subplots_adjust(hspace=0.5)  # –í–º–µ—Å—Ç–æ plt.tight_layout()
st.pyplot(fig)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 3D-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
st.subheader("3D-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
features_3d = [feature for feature, _ in top3_features]
fig_3d = plt.figure(figsize=(10, 8))
ax_3d = fig_3d.add_subplot(111, projection='3d')
markers = {1: 'o', 0: '^'}
colors = {1: 'blue', 0: 'red'}

for cls in data[15].unique():
    data_cls = data[data[15] == cls]
    ax_3d.scatter(
        data_cls[features_3d[0]], data_cls[features_3d[1]], data_cls[features_3d[2]],
        marker=markers[cls], color=colors[cls], label=f"–ö–ª–∞—Å—Å {cls}"
    )

ax_3d.set_title("3D-–≥—Ä–∞—Ñ–∏–∫: –û–¥–æ–±—Ä–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞")
ax_3d.set_xlabel(features_3d[0])
ax_3d.set_ylabel(features_3d[1])
ax_3d.set_zlabel(features_3d[2])
ax_3d.legend()
st.pyplot(fig_3d)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-–∫—Ä–∏–≤—ã—Ö
st.subheader("ROC-–∫—Ä–∏–≤—ã–µ –º–æ–¥–µ–ª–µ–π")
fig_roc, ax_roc = plt.subplots(figsize=(8, 6))
for name, res in results.items():
    if res["roc_auc"] is not None and res["fpr"] is not None and res["tpr"] is not None:
        ax_roc.plot(res["fpr"], res["tpr"], label=f"{name} (AUC = {res['roc_auc']:.3f})")

ax_roc.plot([0, 1], [0, 1], linestyle='--', color='gray', label='–°–ª—É—á–∞–π–Ω–æ–µ —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ')
ax_roc.set_title("ROC-–∫—Ä–∏–≤—ã–µ (—Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞)")
ax_roc.set_xlabel("False Positive Rate")
ax_roc.set_ylabel("True Positive Rate")
ax_roc.legend(loc="lower right")
st.pyplot(fig_roc)
